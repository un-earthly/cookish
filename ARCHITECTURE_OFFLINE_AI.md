# Architecture Overview: Offline AI Implementation

## Component Hierarchy

```
ChatScreen (chat.tsx)
â”œâ”€â”€ Header (Mode indicators)
â”œâ”€â”€ ModelSelector (Model picker modal)
â”‚   â”œâ”€â”€ Available Models List
â”‚   â”œâ”€â”€ Download Progress Bars
â”‚   â””â”€â”€ Delete/Download Actions
â”œâ”€â”€ ScrollView (Messages)
â”‚   â””â”€â”€ ChatInterface (Existing)
â””â”€â”€ ChatComposer (New input component)
    â”œâ”€â”€ ModelSelector Button
    â”œâ”€â”€ Voice Button (optional)
    â”œâ”€â”€ Text Input
    â””â”€â”€ Send Button
```

## Data Flow

### Download Flow
```
User taps "Download"
    â†“
ModelSelector calls llamaService.downloadModel()
    â†“
llamaService downloads from HuggingFace
    â†“
Progress callbacks update UI
    â†“
File saved to device storage
    â†“
Model ready for use
```

### Chat Flow (Offline)
```
User types message
    â†“
ChatComposer.onSend()
    â†“
ChatScreen.handleSendMessage()
    â†“
llamaService.streamCompletion()
    â†“
Tokens stream back in real-time
    â†“
Display in ChatInterface
```

### Chat Flow (Online)
```
User types message
    â†“
ChatComposer.onSend()
    â†“
ChatScreen.handleSendMessage()
    â†“
chatService.processUserMessage()
    â†“
Cloud API call
    â†“
Response returned
    â†“
Display in ChatInterface
```

## Service Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           User Interface Layer              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ ChatScreen  â”‚â—„â”€â”€â”€â”€â–ºâ”‚ModelSelector â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚         â”‚                     â”‚            â”‚
â”‚         â–¼                     â–¼            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ChatComposer â”‚      â”‚ProgressBars  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Service Layer                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚llamaService  â”‚     â”‚chatService  â”‚     â”‚
â”‚  â”‚              â”‚     â”‚             â”‚     â”‚
â”‚  â”‚ - download   â”‚     â”‚ - process   â”‚     â”‚
â”‚  â”‚ - initialize â”‚     â”‚ - history   â”‚     â”‚
â”‚  â”‚ - completion â”‚     â”‚ - sessions  â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Native Layer                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  llama.rn    â”‚     â”‚react-native-â”‚     â”‚
â”‚  â”‚  (LLM Core)  â”‚     â”‚  fs2 (File) â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Device Storage                      â”‚
â”‚  ğŸ“ DocumentDirectory/                      â”‚
â”‚     â”œâ”€â”€ deepseek-r1-1.5b.gguf (0.7 GB)    â”‚
â”‚     â”œâ”€â”€ phi-3-mini-4k.gguf (2.4 GB)       â”‚
â”‚     â””â”€â”€ llama-3.2-1b.gguf (0.8 GB)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## State Management

```typescript
// Chat Screen State
{
  messages: ChatMessage[]           // Chat history
  selectedModel: LlamaModel | null  // Current model
  isModelReady: boolean             // Model initialized
  useOfflineMode: boolean           // Mode flag
  isProcessing: boolean             // Loading state
}

// LlamaService State
{
  context: LlamaContext | null      // Active model
  currentModel: LlamaModel | null   // Loaded model
  downloadProgress: Map<>           // Progress tracking
}

// ModelSelector State
{
  isOpen: boolean                   // Modal visibility
  downloadedModels: Set<string>     // Downloaded IDs
  downloadProgress: Map<>           // Progress by model
  loading: boolean                  // Download state
}
```

## API Surface

### LlamaService
```typescript
// Core Methods
downloadModel(model, onProgress) â†’ Promise<boolean>
initialize(model) â†’ Promise<boolean>
completion(messages, onToken) â†’ Promise<string>
streamCompletion(messages, onToken) â†’ Promise<string>

// Management
isModelDownloaded(model) â†’ Promise<boolean>
deleteModel(model) â†’ Promise<boolean>
getDownloadedModels() â†’ Promise<LlamaModel[]>
cleanup() â†’ Promise<void>

// Info
getCurrentModel() â†’ LlamaModel | null
isInitialized() â†’ boolean
```

### ModelSelector Props
```typescript
{
  selectedModel: LlamaModel | null
  onModelSelect: (model) => void
  disabled?: boolean
}
```

### ChatComposer Props
```typescript
{
  onSend: (message: string) => void
  onVoicePress?: () => void
  selectedModel: LlamaModel | null
  onModelSelect: (model) => void
  disabled?: boolean
  placeholder?: string
  showVoiceButton?: boolean
  showModelSelector?: boolean
}
```

## File Structure

```
cookish/
â”œâ”€â”€ services/
â”‚   â””â”€â”€ llamaService.ts              # 276 lines
â”‚       â”œâ”€â”€ Model download logic
â”‚       â”œâ”€â”€ Initialization & cleanup
â”‚       â”œâ”€â”€ Completion (streaming/batch)
â”‚       â””â”€â”€ Storage management
â”‚
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ ModelSelector.tsx            # 332 lines
â”‚   â”‚   â”œâ”€â”€ Model browser UI
â”‚   â”‚   â”œâ”€â”€ Download progress
â”‚   â”‚   â””â”€â”€ Storage management
â”‚   â”‚
â”‚   â””â”€â”€ ChatComposer.tsx             # 115 lines
â”‚       â”œâ”€â”€ Text input
â”‚       â”œâ”€â”€ Model selector
â”‚       â”œâ”€â”€ Voice button
â”‚       â””â”€â”€ Send button
â”‚
â””â”€â”€ app/(tabs)/
    â””â”€â”€ chat.tsx                     # Updated
        â”œâ”€â”€ Message handling
        â”œâ”€â”€ Mode switching
        â”œâ”€â”€ Model initialization
        â””â”€â”€ UI integration
```

## Dependencies Graph

```
chat.tsx
  â”œâ”€ imports ModelSelector
  â”œâ”€ imports ChatComposer
  â”œâ”€ imports llamaService
  â””â”€ imports chatService (existing)

ModelSelector
  â”œâ”€ imports llamaService
  â””â”€ imports BlurView, icons

ChatComposer
  â”œâ”€ imports ModelSelector
  â””â”€ imports BlurView, TextInput

llamaService
  â”œâ”€ imports llama.rn
  â””â”€ imports expo-file-system
```

## Event Flow

### User Downloads Model
```
1. User taps ModelSelector
2. Modal opens with models list
3. User taps download icon
4. llamaService.downloadModel() called
5. Progress callbacks fire
6. UI updates progress bar
7. Download completes
8. Model marked as downloaded
9. User can select model
```

### User Sends Message (Offline)
```
1. User types in ChatComposer
2. User taps send
3. onSend callback fires
4. handleSendMessage() in chat.tsx
5. Checks: useOfflineMode && isModelReady
6. llamaService.streamCompletion() called
7. Tokens stream back
8. Each token updates UI
9. Complete message saved
10. Chat updates
```

## Memory Management

```
Active States:
- One LlamaContext at a time
- Model files cached on disk
- Messages stored in Supabase

Cleanup Triggers:
- App backgrounded â†’ Release context
- Model switched â†’ Cleanup old, init new
- App closed â†’ Automatic cleanup
```

## Error Handling

```
Download Errors:
- Network failure â†’ Retry prompt
- Storage full â†’ Alert user
- Invalid URL â†’ Log error

Initialization Errors:
- Model corrupt â†’ Redownload prompt
- Low memory â†’ Suggest smaller model
- GPU unavailable â†’ Fallback to CPU

Generation Errors:
- Context overflow â†’ Truncate input
- OOM error â†’ Release and retry
- Timeout â†’ Cancel and retry
```

## Performance Optimizations

1. **Streaming**: Tokens displayed as generated
2. **Lazy Loading**: Models only loaded when selected
3. **Caching**: Downloaded models persist
4. **Memory**: One model at a time
5. **GPU**: Metal/OpenCL acceleration
6. **Quantization**: GGUF format for efficiency

## Security Considerations

- âœ… All processing on-device
- âœ… No data leaves device in offline mode
- âœ… Models from trusted sources (HuggingFace)
- âœ… File permissions properly set
- âœ… User controls model storage

---

This architecture provides a robust, maintainable offline AI system! ğŸš€
